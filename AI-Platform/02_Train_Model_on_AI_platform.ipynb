{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "02 Train Model on AI platform.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommycon/Deeplearning-GoogleCloud/blob/main/AI-Platform/02_Train_Model_on_AI_platform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2GTxMHLV_-",
        "outputId": "8c003e7e-5a6b-4241-cfe7-3bbdf48cdc89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chown: invalid user: ‘jupyter:jupyter’\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWiPIFBgLWAD"
      },
      "source": [
        "# Ensure the right version of Tensorflow is installed.\n",
        "!pip freeze | grep tensorflow==2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoBIP0tmLmOv",
        "outputId": "977d0e93-e569-433c-c0fc-acd207909540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF10Q5FrLWAH"
      },
      "source": [
        "# change these to try this notebook out\n",
        "BUCKET = 'dataflow-tfx-pipeline'\n",
        "PROJECT = 'tc-test-project-260312'\n",
        "REGION = 'us-central1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OldhdlR6LWAK"
      },
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TFVERSION'] = '2.1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTdz2ZChLWAN",
        "outputId": "c70aee89-da35-4a0c-d532-b36fd8517127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu6H7z-ULWAU",
        "outputId": "75a40a79-7992-4198-ef91-30b918272d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%%bash\n",
        "gsutil ls gs://${BUCKET}/babyweight/preproc/*-0000*"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataflow-tfx-pipeline/babyweight/preproc/eval.csv-00000-of-00001\n",
            "gs://dataflow-tfx-pipeline/babyweight/preproc/train.csv-00000-of-00004\n",
            "gs://dataflow-tfx-pipeline/babyweight/preproc/train.csv-00001-of-00004\n",
            "gs://dataflow-tfx-pipeline/babyweight/preproc/train.csv-00002-of-00004\n",
            "gs://dataflow-tfx-pipeline/babyweight/preproc/train.csv-00003-of-00004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ke9kDcMXob",
        "outputId": "933272cb-cf99-44c2-ff3d-2920a4d386ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%bash\n",
        "pwd\n",
        "mkdir -p babyweight/trainer\n",
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "adc.json\n",
            "babyweight\n",
            "babyweight_trained\n",
            "config.yaml\n",
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU-u66eQLWAa",
        "outputId": "53be95b8-d818-446f-f1fc-6b670a69b530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile babyweight/trainer/task.py\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "from . import model\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--bucket',\n",
        "        help = 'GCS path to data. We assume that data is in gs://BUCKET/babyweight/preproc/',\n",
        "        required = True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--output_dir',\n",
        "        help = 'GCS location to write checkpoints and export models',\n",
        "        required = True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--batch_size',\n",
        "        help = 'Number of examples to compute gradient over.',\n",
        "        type = int,\n",
        "        default = 512\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--job-dir',\n",
        "        help = 'this model ignores this field, but it is required by gcloud',\n",
        "        default = 'junk'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--nnsize',\n",
        "        help = 'Hidden layer sizes to use for DNN feature columns -- provide space-separated layers',\n",
        "        nargs = '+',\n",
        "        type = int,\n",
        "        default=[128, 32, 4]\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--nembeds',\n",
        "        help = 'Embedding size of a cross of n key real-valued parameters',\n",
        "        type = int,\n",
        "        default = 3\n",
        "    )\n",
        "\n",
        "    ## TODO 1: add the new arguments here \n",
        "    parser.add_argument(\n",
        "        '--train_examples',\n",
        "        help = 'Number of examples (in thousands) to run the training job over. If this is more than actual # of examples available, it cycles through them. So specifying 1000 here when you have only 100k examples makes this 10 epochs.',\n",
        "        type = int,\n",
        "        default = 5000\n",
        "    )    \n",
        "    parser.add_argument(\n",
        "        '--pattern',\n",
        "        help = 'Specify a pattern that has to be in input files. For example 00001-of will process only one shard',\n",
        "        default = 'of'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--eval_steps',\n",
        "        help = 'Positive number of steps for which to evaluate model. Default to None, which means to evaluate until input_fn raises an end-of-input exception',\n",
        "        type = int,       \n",
        "        default = None\n",
        "    )\n",
        "        \n",
        "    ## parse all arguments\n",
        "    args = parser.parse_args()\n",
        "    arguments = args.__dict__\n",
        "\n",
        "    # unused args provided by service\n",
        "    arguments.pop('job_dir', None)\n",
        "    arguments.pop('job-dir', None)\n",
        "\n",
        "    ## assign the arguments to the model variables\n",
        "    output_dir = arguments.pop('output_dir')\n",
        "    model.BUCKET     = arguments.pop('bucket')\n",
        "    model.BATCH_SIZE = arguments.pop('batch_size')\n",
        "    model.TRAIN_STEPS = (arguments.pop('train_examples') * 100) / model.BATCH_SIZE\n",
        "    model.EVAL_STEPS = arguments.pop('eval_steps')    \n",
        "    print (\"Will train for {} steps using batch_size={}\".format(model.TRAIN_STEPS, model.BATCH_SIZE))\n",
        "    model.PATTERN = arguments.pop('pattern')\n",
        "    model.NEMBEDS= arguments.pop('nembeds')\n",
        "    model.NNSIZE = arguments.pop('nnsize')\n",
        "    print (\"Will use DNN size of {}\".format(model.NNSIZE))\n",
        "\n",
        "    # Append trial_id to path if we are doing hptuning\n",
        "    # This code can be removed if you are not using hyperparameter tuning\n",
        "    output_dir = os.path.join(\n",
        "        output_dir,\n",
        "        json.loads(\n",
        "            os.environ.get('TF_CONFIG', '{}')\n",
        "        ).get('task', {}).get('trial', '')\n",
        "    )\n",
        "\n",
        "    # Run the training job\n",
        "    model.train_and_evaluate(output_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting babyweight/trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-DjK4oELWAd",
        "outputId": "1eac6cde-64eb-4749-9623-547fabaf0864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile babyweight/trainer/model.py\n",
        "import shutil\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "BUCKET = None  # set from task.py\n",
        "PATTERN = 'of' # gets all files\n",
        "\n",
        "# Determine CSV, label, and key columns\n",
        "CSV_COLUMNS = 'weight_pounds,is_male,mother_age,plurality,gestation_weeks,key'.split(',')\n",
        "LABEL_COLUMN = 'weight_pounds'\n",
        "KEY_COLUMN = 'key'\n",
        "\n",
        "# Set default values for each CSV column\n",
        "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], ['nokey']]\n",
        "\n",
        "# Define some hyperparameters\n",
        "TRAIN_STEPS = 10000\n",
        "EVAL_STEPS = None\n",
        "BATCH_SIZE = 512\n",
        "NEMBEDS = 3\n",
        "NNSIZE = [64, 16, 4]\n",
        "\n",
        "# Create an input function reading a file using the Dataset API\n",
        "# Then provide the results to the Estimator API\n",
        "def read_dataset(prefix, mode, batch_size):\n",
        "    def _input_fn():\n",
        "        def decode_csv(value_column):\n",
        "            columns = tf.decode_csv(value_column, record_defaults=DEFAULTS)\n",
        "            features = dict(zip(CSV_COLUMNS, columns))\n",
        "            label = features.pop(LABEL_COLUMN)\n",
        "            return features, label\n",
        "        \n",
        "        # Use prefix to create file path\n",
        "        file_path = 'gs://{}/babyweight/preproc/{}*{}*'.format(BUCKET, prefix, PATTERN)\n",
        "\n",
        "        # Create list of files that match pattern\n",
        "        file_list = tf.gfile.Glob(file_path)\n",
        "\n",
        "        # Create dataset from file list\n",
        "        dataset = (tf.data.TextLineDataset(file_list)  # Read text file\n",
        "                    .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
        "      \n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            num_epochs = None # indefinitely\n",
        "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
        "        else:\n",
        "            num_epochs = 1 # end-of-input after this\n",
        " \n",
        "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
        "        return dataset.make_one_shot_iterator().get_next()\n",
        "    return _input_fn\n",
        "\n",
        "# Define feature columns\n",
        "def get_wide_deep():\n",
        "    # Define column types\n",
        "    is_male,mother_age,plurality,gestation_weeks = \\\n",
        "        [\\\n",
        "            tf.feature_column.categorical_column_with_vocabulary_list('is_male', \n",
        "                        ['True', 'False', 'Unknown']),\n",
        "            tf.feature_column.numeric_column('mother_age'),\n",
        "            tf.feature_column.categorical_column_with_vocabulary_list('plurality',\n",
        "                        ['Single(1)', 'Twins(2)', 'Triplets(3)',\n",
        "                         'Quadruplets(4)', 'Quintuplets(5)','Multiple(2+)']),\n",
        "            tf.feature_column.numeric_column('gestation_weeks')\n",
        "        ]\n",
        "\n",
        "    # Discretize\n",
        "    age_buckets = tf.feature_column.bucketized_column(mother_age, \n",
        "                        boundaries=np.arange(15,45,1).tolist())\n",
        "    gestation_buckets = tf.feature_column.bucketized_column(gestation_weeks, \n",
        "                        boundaries=np.arange(17,47,1).tolist())\n",
        "      \n",
        "    # Sparse columns are wide, have a linear relationship with the output\n",
        "    wide = [is_male,\n",
        "            plurality,\n",
        "            age_buckets,\n",
        "            gestation_buckets]\n",
        "    \n",
        "    # Feature cross all the wide columns and embed into a lower dimension\n",
        "    crossed = tf.feature_column.crossed_column(wide, hash_bucket_size=20000)\n",
        "    embed = tf.feature_column.embedding_column(crossed, NEMBEDS)\n",
        "    \n",
        "    # Continuous columns are deep, have a complex relationship with the output\n",
        "    deep = [mother_age,\n",
        "            gestation_weeks,\n",
        "            embed]\n",
        "    return wide, deep\n",
        "\n",
        "# Create serving input function to be able to serve predictions later using provided inputs\n",
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "        'is_male': tf.placeholder(tf.string, [None]),\n",
        "        'mother_age': tf.placeholder(tf.float32, [None]),\n",
        "        'plurality': tf.placeholder(tf.string, [None]),\n",
        "        'gestation_weeks': tf.placeholder(tf.float32, [None]),\n",
        "        KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])\n",
        "    }\n",
        "    features = {\n",
        "        key: tf.expand_dims(tensor, -1)\n",
        "        for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
        "\n",
        "# create metric for hyperparameter tuning\n",
        "def my_rmse(labels, predictions):\n",
        "    pred_values = predictions['predictions']\n",
        "    return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
        "\n",
        "def forward_features(estimator, key):\n",
        "    def new_model_fn(features, labels, mode, config):\n",
        "        spec = estimator.model_fn(features, labels, mode, config)\n",
        "        predictions = spec.predictions\n",
        "        predictions[key] = features[key]\n",
        "        spec = spec._replace(predictions=predictions)\n",
        "        return spec\n",
        "    return tf.estimator.Estimator(model_fn=new_model_fn, model_dir=estimator.model_dir, config=estimator.config)\n",
        "\n",
        "# Create estimator to train and evaluate\n",
        "def train_and_evaluate(output_dir):\n",
        "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
        "    wide, deep = get_wide_deep()\n",
        "    EVAL_INTERVAL = 300 # seconds\n",
        "\n",
        "    ## TODO 2a: set the save_checkpoints_secs to the EVAL_INTERVAL\n",
        "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = EVAL_INTERVAL,\n",
        "                                        keep_checkpoint_max = 3)\n",
        "    \n",
        "    ## TODO 2b: change the dnn_hidden_units to NNSIZE\n",
        "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
        "        model_dir = output_dir,\n",
        "        linear_feature_columns = wide,\n",
        "        dnn_feature_columns = deep,\n",
        "        dnn_hidden_units = NNSIZE,\n",
        "        config = run_config)\n",
        "    \n",
        "    # illustrates how to add an extra metric\n",
        "    estimator = tf.estimator.add_metrics(estimator, my_rmse)\n",
        "    # for batch prediction, you need a key associated with each instance\n",
        "    estimator = forward_features(estimator, KEY_COLUMN)\n",
        "    \n",
        "    ## TODO 2c: Set the third argument of read_dataset to BATCH_SIZE \n",
        "    ## TODO 2d: and set max_steps to TRAIN_STEPS\n",
        "    train_spec = tf.estimator.TrainSpec(\n",
        "        input_fn = read_dataset('train', tf.estimator.ModeKeys.TRAIN, BATCH_SIZE),\n",
        "        max_steps = TRAIN_STEPS)\n",
        "    \n",
        "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn, exports_to_keep=1)\n",
        "\n",
        "    ## TODO 2e: Lastly, set steps equal to EVAL_STEPS\n",
        "    eval_spec = tf.estimator.EvalSpec(\n",
        "        input_fn = read_dataset('eval', tf.estimator.ModeKeys.EVAL, 2**15),  # no need to batch in eval\n",
        "        steps = EVAL_STEPS,\n",
        "        start_delay_secs = 60, # start evaluating after N seconds\n",
        "        throttle_secs = EVAL_INTERVAL,  # evaluate every N seconds\n",
        "        exporters = exporter)\n",
        "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing babyweight/trainer/model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w25STi0PLWAg",
        "outputId": "574304c5-9e90-4854-f3d4-4203b3b0ed30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%bash\n",
        "echo \"bucket=${BUCKET}\"\n",
        "rm -rf babyweight_trained\n",
        "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
        "python -m trainer.task \\\n",
        "  --bucket=${BUCKET} \\\n",
        "  --output_dir=babyweight_trained \\\n",
        "  --job-dir=./tmp \\\n",
        "  --pattern=\"00000-of-\" --train_examples=1 --eval_steps=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bucket=dataflow-tfx-pipeline\n",
            "Will train for 0.1953125 steps using batch_size=512\n",
            "Will use DNN size of [128, 32, 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-08-27 08:11:16.617553: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'babyweight_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'babyweight_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'babyweight_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /content/babyweight/trainer/model.py:56: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/linear.py:1481: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "2020-08-27 08:11:21.524016: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-27 08:11:21.524342: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b78680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-27 08:11:21.524418: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-27 08:11:21.563087: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-27 08:11:21.626210: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-08-27 08:11:21.626335: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (4f75f25edbf1): /proc/driver/nvidia/version does not exist\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
            "INFO:tensorflow:Saving checkpoints for 0 into babyweight_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
            "INFO:tensorflow:loss = 31207.84, step = 1\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1...\n",
            "INFO:tensorflow:Saving checkpoints for 1 into babyweight_trained/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1...\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-08-27T08:11:25Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Evaluation [1/1]\n",
            "INFO:tensorflow:Inference Time : 2.45204s\n",
            "INFO:tensorflow:Finished evaluation at 2020-08-27-08:11:27\n",
            "INFO:tensorflow:Saving dict for global step 1: average_loss = 34.215622, global_step = 1, label/mean = 6.9253216, loss = 1121177.5, prediction/mean = 1.2017062, rmse = 5.849412\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1: babyweight_trained/model.ckpt-1\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
            "INFO:tensorflow:'serving_default' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>}\n",
            "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'is_male': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=string>, 'mother_age': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>, 'plurality': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=string>, 'gestation_weeks': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float32>, 'key': <tf.Tensor 'PlaceholderWithDefault:0' shape=(?,) dtype=string>}\n",
            "WARNING:tensorflow:Export includes no default signature!\n",
            "INFO:tensorflow:Restoring parameters from babyweight_trained/model.ckpt-1\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: babyweight_trained/export/exporter/temp-1598515887/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 31207.84.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzqfdMj9OvrJ",
        "outputId": "9e202263-0332-4391-9231-ca6987fcfa87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%bash\n",
        "ls babyweight_trained/export/exporter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1598515887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywe4nLyrPTtK",
        "outputId": "7fbbd9d8-4e5b-4c01-b6dc-53550539a34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/babyweight/trainer/__init__.py\n",
        "\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing /content/babyweight/trainer/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EglFS2G38WFZ"
      },
      "source": [
        "#Need to set config of 1 evaluator machine due to changes in the change to VF1 -> VF2\n",
        "#https://stackoverflow.com/questions/62337037/ai-platform-no-eval-folder-or-export-folder-in-outputs-when-running-tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esEoXiJ8xlMw",
        "outputId": "4f87c2f4-2366-43f8-89cc-5e7f5bd72d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  masterType: standard\n",
        "  workerType: standard\n",
        "  workerCount: 1\n",
        "  evaluatorType: standard\n",
        "  evaluatorCount: 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing config.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-He4J1aKLWAu",
        "outputId": "046c629d-43a0-4928-f83c-baf73cf7091f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
        "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "  --region=$REGION \\\n",
        "  --module-name=trainer.task \\\n",
        "  --package-path=$(pwd)/babyweight/trainer \\\n",
        "  --job-dir=$OUTDIR \\\n",
        "  --staging-bucket=gs://$BUCKET \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7 \\\n",
        "  --config=config.yaml \\\n",
        "  -- \\\n",
        "  --bucket=${BUCKET} \\\n",
        "  --output_dir=${OUTDIR} \\\n",
        "  --train_examples=20000  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataflow-tfx-pipeline/babyweight/trained_model us-central1 babyweight_200827_081146\n",
            "jobId: babyweight_200827_081146\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/#1598463402102495...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/checkpoint#1598463442127239...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/#1598463402429002...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/#1598463281836334...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/eval/events.out.tfevents.1598463399.gke-cml-0826-172718--n1-standard-4-0a-ea72deef-jgxs#1598463401254192...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/#1598463410122354...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/eval/#1598463399854243...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/variables/#1598463410729283...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/events.out.tfevents.1598463281.gke-cml-0826-172718--n1-standard-4-0a-ea72deef-1j92#1598463443725541...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/saved_model.pb#1598463410434997...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/variables/variables.data-00000-of-00002#1598463411079495...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/variables/variables.data-00001-of-00002#1598463411421719...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/export/exporter/1598463401/variables/variables.index#1598463411728467...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/graph.pbtxt#1598463284482998...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-0.data-00000-of-00002#1598463288226886...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-0.data-00001-of-00002#1598463287821339...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-0.index#1598463288621657...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-3907.data-00000-of-00002#1598463440668661...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-0.meta#1598463290572288...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-3907.data-00001-of-00002#1598463440213933...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-3907.index#1598463441030767...\n",
            "Removing gs://dataflow-tfx-pipeline/babyweight/trained_model/model.ckpt-3907.meta#1598463443237226...\n",
            "/ [1/22 objects]   4% Done                                                      \r/ [2/22 objects]   9% Done                                                      \r/ [3/22 objects]  13% Done                                                      \r/ [4/22 objects]  18% Done                                                      \r/ [5/22 objects]  22% Done                                                      \r/ [6/22 objects]  27% Done                                                      \r/ [7/22 objects]  31% Done                                                      \r/ [8/22 objects]  36% Done                                                      \r/ [9/22 objects]  40% Done                                                      \r/ [10/22 objects]  45% Done                                                     \r/ [11/22 objects]  50% Done                                                     \r/ [12/22 objects]  54% Done                                                     \r/ [13/22 objects]  59% Done                                                     \r/ [14/22 objects]  63% Done                                                     \r/ [15/22 objects]  68% Done                                                     \r/ [16/22 objects]  72% Done                                                     \r/ [17/22 objects]  77% Done                                                     \r/ [18/22 objects]  81% Done                                                     \r/ [19/22 objects]  86% Done                                                     \r/ [20/22 objects]  90% Done                                                     \r/ [21/22 objects]  95% Done                                                     \r/ [22/22 objects] 100% Done                                                     \r\n",
            "Operation completed over 22 objects.                                             \n",
            "Job [babyweight_200827_081146] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe babyweight_200827_081146\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs babyweight_200827_081146\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJQPrjohLWA0"
      },
      "source": [
        "<h2> Repeat training </h2>\n",
        "<p>\n",
        "This time with tuned parameters (note last line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgE7fdayLWA1",
        "outputId": "bf4ab8b0-952b-49f6-8c44-9d0d3c14307a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tunedv2\n",
        "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "  --region=$REGION \\\n",
        "  --module-name=trainer.task \\\n",
        "  --package-path=$(pwd)/babyweight/trainer \\\n",
        "  --job-dir=$OUTDIR \\\n",
        "  --staging-bucket=gs://$BUCKET \\\n",
        "  --runtime-version=2.1 \\\n",
        "  --python-version=3.7 \\\n",
        "  --config=config.yaml \\\n",
        "  -- \\\n",
        "  --bucket=${BUCKET} \\\n",
        "  --output_dir=${OUTDIR} \\\n",
        "  --train_examples=2000 --batch_size=35 --nembeds=16 --nnsize=281"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tunedv2 us-central1 babyweight_200827_083955\n",
            "jobId: babyweight_200827_083955\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "CommandException: 1 files/objects could not be removed.\n",
            "Job [babyweight_200827_083955] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe babyweight_200827_083955\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs babyweight_200827_083955\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhOFMTgFXn9e",
        "outputId": "962c6767-b98e-4f31-87f9-c4f5994bbca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "%%bash\n",
        "gsutil ls gs://dataflow-tfx-pipeline/babyweight/trained_model_tunedv2\n",
        "\n",
        "#Model not exporting due to changes from TF1 to TF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/checkpoint\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/events.out.tfevents.1598516669.gke-cml-0827-081152--n1-standard-4-0a-34315352-c82c\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/graph.pbtxt\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-0.data-00000-of-00002\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-0.data-00001-of-00002\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-0.index\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-0.meta\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-5715.data-00000-of-00002\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-5715.data-00001-of-00002\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-5715.index\n",
            "gs://dataflow-tfx-pipeline/babyweight/trained_model_tuned/model.ckpt-5715.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8oPlGVJaX46"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}