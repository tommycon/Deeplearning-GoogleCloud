{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "BQ to Train Keras on AI Platform - Model Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommycon/Deeplearning-GoogleCloud/blob/main/BigQuery-Keras-AI-Platform/BQ_to_Train_Keras_on_AI_Platform_Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lgXaIisUjdd"
      },
      "source": [
        "# Ensure the right version of Tensorflow is installed.\n",
        "!pip freeze | grep tensorflow==2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X8o30yeU2Rm",
        "outputId": "86df5f2b-25b3-46ad-86ba-ffe6cdc21bd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "b_f9rgkeUjdu"
      },
      "source": [
        "# change these to try this notebook out\n",
        "BUCKET = 'bq-deploy-ml-pipeline'\n",
        "PROJECT = 'tc-test-project-260312'\n",
        "REGION = 'us-central1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3AnfP4rUjeC"
      },
      "source": [
        "import os\n",
        "os.environ[\"PROJECT\"] = PROJECT\n",
        "os.environ[\"BUCKET\"] = BUCKET\n",
        "os.environ[\"REGION\"] = REGION\n",
        "os.environ[\"TFVERSION\"] = \"2.1\"\n",
        "os.environ[\"PYTHONVERSION\"] = \"3.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZUQtASG10xO",
        "outputId": "a180c93e-9ed7-420b-e09f-4051b3311ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%bash\n",
        "sudo pip freeze | grep google-cloud-bigquery==1.6.1 || \\\n",
        "sudo pip install google-cloud-bigquery==1.6.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "google-cloud-bigquery==1.6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement hypertune (from versions: none)\n",
            "ERROR: No matching distribution found for hypertune\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWqKH8NLVqKa",
        "outputId": "e5b37ff7-4cf2-4584-a04d-f3b90bc21fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION\n",
        "#gsutil mb gs://bq-deploy-ml-pipeline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSPBbZgCUjeU"
      },
      "source": [
        "import os\n",
        "from google.cloud import bigquery"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOAwhNiPUjee",
        "outputId": "156eeb33-7496-4cf2-e186-2c81ed5017e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "# Create a BigQuery dataset for babyweight if it doesn't exist\n",
        "datasetexists=$(bq ls -d | grep -w citibike)\n",
        "\n",
        "if [ -n \"$datasetexists\" ]; then\n",
        "    echo -e \"BigQuery dataset already exists, let's not recreate it.\"\n",
        "\n",
        "else\n",
        "    echo \"Creating BigQuery dataset titled: babyweight\"\n",
        "    \n",
        "    bq --location=US mk --dataset \\\n",
        "        --description \"citibike\" \\\n",
        "        $PROJECT:citibike\n",
        "    echo \"Here are your current datasets:\"\n",
        "    bq ls\n",
        "fi\n",
        "    \n",
        "## Create GCS bucket if it doesn't exist already...\n",
        "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
        "\n",
        "if [ -n \"$exists\" ]; then\n",
        "    echo -e \"Bucket exists, let's not recreate it.\"\n",
        "    \n",
        "else\n",
        "    echo \"Creating a new GCS bucket.\"\n",
        "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "    echo \"Here are your current buckets:\"\n",
        "    gsutil ls\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating BigQuery dataset titled: babyweight\n",
            "Dataset 'tc-test-project-260312:citibike' successfully created.\n",
            "Here are your current datasets:\n",
            "   datasetId    \n",
            " -------------- \n",
            "  babyweight    \n",
            "  bqml          \n",
            "  citibike      \n",
            "  fastly_logs   \n",
            "  weather_demo  \n",
            "Bucket exists, let's not recreate it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYvYKS8bUjei",
        "outputId": "c503e225-a967-4bbb-bad1-163e6abbb553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "CREATE OR REPLACE TABLE\n",
        "    citibike.citibike_data AS\n",
        "SELECT\n",
        "    tripduration,\n",
        "    starttime,\n",
        "    start_station_latitude,\n",
        "    start_station_longitude,\n",
        "    end_station_latitude,\n",
        "    end_station_longitude,\n",
        "    usertype,\n",
        "    birth_year,\n",
        "    gender\n",
        "FROM\n",
        "bigquery-public-data.new_york_citibike.citibike_trips\n",
        "WHERE starttime > date_add(CURRENT_DATE(), interval -3 year)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4_rtN7rUjem"
      },
      "source": [
        "### Augment dataset to simulate missing data\n",
        "\n",
        "Now we want to augment our dataset with our simulated babyweight data by setting all gender information to `Unknown` and setting plurality of all non-single births to `Multiple(2+)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e9Ukuo5Ujem",
        "outputId": "d4b4d6d3-2add-49c8-84ac-c7ed5bbdf6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "CREATE OR REPLACE TABLE\n",
        "    citibike.citibike_augmented_data AS\n",
        "SELECT\n",
        "    tripduration,\n",
        "    sqrt(pow((start_station_latitude -  end_station_latitude),2) + pow((start_station_longitude - end_station_longitude),2)) as euclidean_diff,\n",
        "    usertype,\n",
        "    birth_year,\n",
        "    gender,\n",
        "    FARM_FINGERPRINT(\n",
        "        CONCAT(\n",
        "            CAST(EXTRACT(YEAR FROM starttime) AS STRING),\n",
        "            CAST(EXTRACT(MONTH FROM starttime) AS STRING)\n",
        "        )\n",
        "    ) AS hashmonth\n",
        "FROM\n",
        "    citibike.citibike_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFIszTaUjer"
      },
      "source": [
        "#### Split augmented dataset into train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMNRractvREL",
        "outputId": "c22bd7f9-7198-4caa-e917-888fad45c212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "CREATE OR REPLACE TABLE\n",
        "    citibike.citibike_data_train AS\n",
        "SELECT\n",
        "    tripduration,\n",
        "    euclidean_diff,\n",
        "    usertype,\n",
        "    birth_year,\n",
        "    gender,\n",
        "FROM\n",
        "    citibike.citibike_augmented_data\n",
        "WHERE\n",
        "    ABS(MOD(hashmonth, 4)) < 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSFYpaYaUjey"
      },
      "source": [
        "#### Split augmented dataset into eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaY2uAgIUjez",
        "outputId": "aac29612-a87e-40c2-98d4-9d7c4d476037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "CREATE OR REPLACE TABLE\n",
        "    citibike.citibike_data_eval AS\n",
        "SELECT\n",
        "    tripduration,\n",
        "    euclidean_diff,\n",
        "    usertype,\n",
        "    birth_year,\n",
        "    gender,\n",
        "FROM\n",
        "    citibike.citibike_augmented_data\n",
        "WHERE\n",
        "    ABS(MOD(hashmonth, 4)) = 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clnaaqQsXkwC"
      },
      "source": [
        "## Verify table creation\n",
        "\n",
        "Verify that you created the dataset and training data table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT_S8R8RUje6",
        "outputId": "5fd019ad-17d3-4117-a3fb-0c70e5d153b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
        "SELECT * FROM citibike.citibike_data_train\n",
        "LIMIT 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripduration</th>\n",
              "      <th>euclidean_diff</th>\n",
              "      <th>usertype</th>\n",
              "      <th>birth_year</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>441</td>\n",
              "      <td>0.011829</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1952</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1799</td>\n",
              "      <td>0.027878</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1949</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>488</td>\n",
              "      <td>0.019923</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1900</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>107</td>\n",
              "      <td>0.003452</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1952</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016</td>\n",
              "      <td>0.025544</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1953</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>427</td>\n",
              "      <td>0.011850</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1945</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>493</td>\n",
              "      <td>0.012062</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1946</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>458</td>\n",
              "      <td>0.014109</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1952</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>603</td>\n",
              "      <td>0.011565</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1948</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>603</td>\n",
              "      <td>0.025594</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>2001</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tripduration  euclidean_diff    usertype  birth_year  gender\n",
              "0           441        0.011829  Subscriber        1952    male\n",
              "1          1799        0.027878  Subscriber        1949    male\n",
              "2           488        0.019923  Subscriber        1900    male\n",
              "3           107        0.003452  Subscriber        1952    male\n",
              "4          2016        0.025544  Subscriber        1953    male\n",
              "5           427        0.011850  Subscriber        1945    male\n",
              "6           493        0.012062  Subscriber        1946  female\n",
              "7           458        0.014109  Subscriber        1952    male\n",
              "8           603        0.011565  Subscriber        1948    male\n",
              "9           603        0.025594  Subscriber        2001    male"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovpwkgpYUje9",
        "outputId": "92d7b0a2-c4ef-402e-93d2-3b9221de76ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "%%bigquery --project {PROJECT}\n",
        "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
        "SELECT * FROM citibike.citibike_data_eval\n",
        "LIMIT 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripduration</th>\n",
              "      <th>euclidean_diff</th>\n",
              "      <th>usertype</th>\n",
              "      <th>birth_year</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>857</td>\n",
              "      <td>0.024528</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1949</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1348</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1951</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>510</td>\n",
              "      <td>0.010083</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1946</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>99</td>\n",
              "      <td>0.003223</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1949</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>223</td>\n",
              "      <td>0.011211</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>2000</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>669</td>\n",
              "      <td>0.015776</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1947</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>121</td>\n",
              "      <td>0.002470</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1945</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>503</td>\n",
              "      <td>0.009004</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1941</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>305</td>\n",
              "      <td>0.012938</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>2000</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1839</td>\n",
              "      <td>0.052289</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1951</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tripduration  euclidean_diff    usertype  birth_year gender\n",
              "0           857        0.024528  Subscriber        1949   male\n",
              "1          1348        0.000000  Subscriber        1951   male\n",
              "2           510        0.010083  Subscriber        1946   male\n",
              "3            99        0.003223  Subscriber        1949   male\n",
              "4           223        0.011211  Subscriber        2000   male\n",
              "5           669        0.015776  Subscriber        1947   male\n",
              "6           121        0.002470  Subscriber        1945   male\n",
              "7           503        0.009004  Subscriber        1941   male\n",
              "8           305        0.012938  Subscriber        2000   male\n",
              "9          1839        0.052289  Subscriber        1951   male"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE6J_5YIUjfC",
        "outputId": "a97f9e25-cc10-448d-c464-b21e879944d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client(project = PROJECT)\n",
        "\n",
        "dataset_name = \"citibike\"\n",
        "\n",
        "# Create dataset reference object\n",
        "dataset_ref = client.dataset(\n",
        "    dataset_id=dataset_name, project=client.project)\n",
        "\n",
        "# Export both train and eval tables\n",
        "for step in [\"train\", \"eval\"]:\n",
        "    destination_uri = os.path.join(\n",
        "        \"gs://\", BUCKET, dataset_name, \"data\", \"{}*.csv\".format(step))\n",
        "    table_name = \"citibike_data_{}\".format(step)\n",
        "    table_ref = dataset_ref.table(table_name)\n",
        "    extract_job = client.extract_table(\n",
        "        table_ref,\n",
        "        destination_uri,\n",
        "        # Location must match that of the source table.\n",
        "        location=\"US\",\n",
        "    )  # API request\n",
        "    extract_job.result()  # Waits for job to complete.\n",
        "\n",
        "    print(\"Exported {}:{}.{} to {}\".format(\n",
        "        client.project, dataset_name, table_name, destination_uri))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exported tc-test-project-260312:citibike.citibike_data_train to gs://bq-deploy-ml-pipeline/citibike/data/train*.csv\n",
            "Exported tc-test-project-260312:citibike.citibike_data_eval to gs://bq-deploy-ml-pipeline/citibike/data/eval*.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxHYiGhVUjfH",
        "outputId": "c78bb2bc-7ff1-41b7-d37b-0acddd1481b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%bash\n",
        "gsutil ls gs://${BUCKET}/citibike/data/*.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://bq-deploy-ml-pipeline/citibike/data/eval000000000000.csv\n",
            "gs://bq-deploy-ml-pipeline/citibike/data/train000000000000.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FBFjZfTUjfV"
      },
      "source": [
        "%%bash\n",
        "mkdir -p citibike/trainer\n",
        "touch citibike/trainer/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5S6RG6BUjfa",
        "outputId": "e8b62636-2a92-46f5-d4de-b40489466a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile citibike/trainer/task.py\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "\n",
        "from trainer import model\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--job-dir\",\n",
        "        help=\"this model ignores this field, but it is required by gcloud\",\n",
        "        default=\"junk\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_data_path\",\n",
        "        help=\"GCS location of training data\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_data_path\",\n",
        "        help=\"GCS location of evaluation data\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        help=\"GCS location to write checkpoints and export models\",\n",
        "        required=True\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        help=\"Number of examples to compute gradient over.\",\n",
        "        type=int,\n",
        "        default=512\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nnsize\",\n",
        "        help=\"Hidden layer sizes for DNN -- provide space-separated layers\",\n",
        "        nargs=\"+\",\n",
        "        type=int,\n",
        "        default=[128, 32, 4]\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nembeds\",\n",
        "        help=\"Embedding size of a cross of n key real-valued parameters\",\n",
        "        type=int,\n",
        "        default=3\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_epochs\",\n",
        "        help=\"Number of epochs to train the model.\",\n",
        "        type=int,\n",
        "        default=10\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_examples\",\n",
        "        help=\"\"\"Number of examples (in thousands) to run the training job over.\n",
        "        If this is more than actual # of examples available, it cycles through\n",
        "        them. So specifying 1000 here when you have only 100k examples makes\n",
        "        this 10 epochs.\"\"\",\n",
        "        type=int,\n",
        "        default=5000\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_steps\",\n",
        "        help=\"\"\"Positive number of steps for which to evaluate model. Default\n",
        "        to None, which means to evaluate until input_fn raises an end-of-input\n",
        "        exception\"\"\",\n",
        "        type=int,\n",
        "        default=None\n",
        "    )\n",
        "\n",
        "    # Parse all arguments\n",
        "    args = parser.parse_args()\n",
        "    arguments = args.__dict__\n",
        "\n",
        "    # Unused args provided by service\n",
        "    arguments.pop(\"job_dir\", None)\n",
        "    arguments.pop(\"job-dir\", None)\n",
        "\n",
        "    # Modify some arguments\n",
        "    arguments[\"train_examples\"] *= 1000\n",
        "\n",
        "    # Append trial_id to path if we are doing hptuning\n",
        "    # This code can be removed if you are not using hyperparameter tuning\n",
        "    arguments[\"output_dir\"] = os.path.join(\n",
        "        arguments[\"output_dir\"],\n",
        "        json.loads(\n",
        "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
        "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
        "    )\n",
        "\n",
        "    # Run the training job\n",
        "    model.train_and_evaluate(arguments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting citibike/trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKN8Lt6VUjfd",
        "outputId": "32b5f962-7256-45c9-cf03-b5296becafbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile citibike/trainer/model.py\n",
        "import datetime\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import hypertune\n",
        "\n",
        "# Determine CSV, label, and key columns\n",
        "CSV_COLUMNS = [\"tripduration\",\n",
        "                \"euclidean_diff\",\n",
        "                \"usertype\",\n",
        "                \"birth_year\",\n",
        "                \"gender\"]\n",
        "LABEL_COLUMN = \"tripduration\"\n",
        "\n",
        "# Set default values for each CSV column.\n",
        "# Treat is_male and plurality as strings.\n",
        "DEFAULTS = [[0.0], [0.0], [\"null\"], [0.0], [\"null\"]]\n",
        "\n",
        "\n",
        "def features_and_labels(row_data):\n",
        "    \"\"\"Splits features and labels from feature dictionary.\n",
        "\n",
        "    Args:\n",
        "        row_data: Dictionary of CSV column names and tensor values.\n",
        "    Returns:\n",
        "        Dictionary of feature tensors and label tensor.\n",
        "    \"\"\"\n",
        "    label = row_data.pop(LABEL_COLUMN)\n",
        "\n",
        "    return row_data, label  # features, label\n",
        "\n",
        "\n",
        "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
        "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
        "\n",
        "    Args:\n",
        "        pattern: str, file pattern to glob into list of files.\n",
        "        batch_size: int, the number of examples per batch.\n",
        "        mode: 'train' | 'eval' to determine if training or evaluating.\n",
        "    Returns:\n",
        "        `Dataset` object.\n",
        "    \"\"\"\n",
        "    print(\"mode = {}\".format(mode))\n",
        "    # Make a CSV dataset\n",
        "    dataset = tf.data.experimental.make_csv_dataset(\n",
        "        file_pattern=pattern,\n",
        "        batch_size=batch_size,\n",
        "        column_names=CSV_COLUMNS,\n",
        "        column_defaults=DEFAULTS)\n",
        "\n",
        "    # Map dataset to features and label\n",
        "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
        "\n",
        "    # Shuffle and repeat for training\n",
        "    if mode == 'train':\n",
        "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
        "\n",
        "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
        "    dataset = dataset.prefetch(buffer_size=1)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def create_input_layers():\n",
        "    \"\"\"Creates dictionary of input layers for each feature.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
        "    \"\"\"\n",
        "    deep_inputs = {\n",
        "        colname: tf.keras.layers.Input(\n",
        "            name=colname, shape=(), dtype=\"float32\")\n",
        "        for colname in [\"euclidean_diff\",\"birth_year\"]\n",
        "    }\n",
        "\n",
        "    wide_inputs = {\n",
        "        colname: tf.keras.layers.Input(\n",
        "            name=colname, shape=(), dtype=\"string\")\n",
        "        for colname in [\"usertype\", \"gender\"]\n",
        "    }\n",
        "\n",
        "    inputs = {**wide_inputs, **deep_inputs}\n",
        "\n",
        "    return inputs\n",
        "\n",
        "\n",
        "def categorical_fc(name, values):\n",
        "    \"\"\"Helper function to wrap categorical feature by indicator column.\n",
        "\n",
        "    Args:\n",
        "        name: str, name of feature.\n",
        "        values: list, list of strings of categorical values.\n",
        "    Returns:\n",
        "        Categorical and indicator column of categorical feature.\n",
        "    \"\"\"\n",
        "    cat_column = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "            key=name, vocabulary_list=values)\n",
        "    ind_column = tf.feature_column.indicator_column(\n",
        "        categorical_column=cat_column)\n",
        "\n",
        "    return cat_column, ind_column\n",
        "\n",
        "\n",
        "def create_feature_columns(nembeds):\n",
        "    \"\"\"Creates wide and deep dictionaries of feature columns from inputs.\n",
        "\n",
        "    Args:\n",
        "        nembeds: int, number of dimensions to embed categorical column down to.\n",
        "    Returns:\n",
        "        Wide and deep dictionaries of feature columns.\n",
        "    \"\"\"\n",
        "    deep_fc = {\n",
        "        colname: tf.feature_column.numeric_column(key=colname)\n",
        "        for colname in [\"euclidean_diff\",\"birth_year\"]\n",
        "    }\n",
        "    wide_fc = {}\n",
        "    is_male, wide_fc[\"usertype\"] = categorical_fc(\n",
        "        \"usertype\", [\"Customer\", \"Subscriber\"])\n",
        "    plurality, wide_fc[\"gender\"] = categorical_fc(\n",
        "        \"gender\", [\"male\", \"female\"])\n",
        "\n",
        "    # Bucketize the float fields. This makes them wide\n",
        "    birth_buckets = tf.feature_column.bucketized_column(\n",
        "        source_column=deep_fc[\"birth_year\"],\n",
        "        boundaries=np.arange(1900, 2020, 5).tolist())\n",
        "    wide_fc[\"birth_buckets\"] = tf.feature_column.indicator_column(\n",
        "        categorical_column=birth_buckets)\n",
        "\n",
        "    euclidean_buckets = tf.feature_column.bucketized_column(\n",
        "        source_column=deep_fc[\"euclidean_diff\"],\n",
        "        boundaries=np.arange(0, 3, .0001).tolist())\n",
        "    wide_fc[\"euclidean_buckets\"] = tf.feature_column.indicator_column(\n",
        "        categorical_column=euclidean_buckets)\n",
        "\n",
        "    # Cross all the wide columns, have to do the crossing before we one-hot\n",
        "    crossed = tf.feature_column.crossed_column(\n",
        "        keys=[birth_buckets, euclidean_buckets],\n",
        "        hash_bucket_size=1000)\n",
        "    deep_fc[\"crossed_embeds\"] = tf.feature_column.embedding_column(\n",
        "        categorical_column=crossed, dimension=nembeds)\n",
        "\n",
        "    return wide_fc, deep_fc\n",
        "\n",
        "\n",
        "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units):\n",
        "    \"\"\"Creates model architecture and returns outputs.\n",
        "\n",
        "    Args:\n",
        "        wide_inputs: Dense tensor used as inputs to wide side of model.\n",
        "        deep_inputs: Dense tensor used as inputs to deep side of model.\n",
        "        dnn_hidden_units: List of integers where length is number of hidden\n",
        "            layers and ith element is the number of neurons at ith layer.\n",
        "    Returns:\n",
        "        Dense tensor output from the model.\n",
        "    \"\"\"\n",
        "    # Hidden layers for the deep side\n",
        "    layers = [int(x) for x in dnn_hidden_units]\n",
        "    deep = deep_inputs\n",
        "    for layerno, numnodes in enumerate(layers):\n",
        "        deep = tf.keras.layers.Dense(\n",
        "            units=numnodes,\n",
        "            activation=\"relu\",\n",
        "            name=\"dnn_{}\".format(layerno+1))(deep)\n",
        "    deep_out = deep\n",
        "\n",
        "    # Linear model for the wide side\n",
        "    wide_out = tf.keras.layers.Dense(\n",
        "        units=10, activation=\"relu\", name=\"linear\")(wide_inputs)\n",
        "\n",
        "    # Concatenate the two sides\n",
        "    both = tf.keras.layers.concatenate(\n",
        "        inputs=[deep_out, wide_out], name=\"both\")\n",
        "\n",
        "    # Final output is a linear activation because this is regression\n",
        "    output = tf.keras.layers.Dense(\n",
        "        units=1, activation=\"linear\", name=\"time\")(both)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    \"\"\"Calculates RMSE evaluation metric.\n",
        "\n",
        "    Args:\n",
        "        y_true: tensor, true labels.\n",
        "        y_pred: tensor, predicted labels.\n",
        "    Returns:\n",
        "        Tensor with value of RMSE between true and predicted labels.\n",
        "    \"\"\"\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "\n",
        "def build_wide_deep_model(dnn_hidden_units=[64, 32], nembeds=3):\n",
        "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
        "\n",
        "    Returns:\n",
        "        `tf.keras.models.Model` object.\n",
        "    \"\"\"\n",
        "    # Create input layers\n",
        "    inputs = create_input_layers()\n",
        "\n",
        "    # Create feature columns for both wide and deep\n",
        "    wide_fc, deep_fc = create_feature_columns(nembeds)\n",
        "\n",
        "    # The constructor for DenseFeatures takes a list of numeric columns\n",
        "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
        "    wide_inputs = tf.keras.layers.DenseFeatures(\n",
        "        feature_columns=wide_fc.values(), name=\"wide_inputs\")(inputs)\n",
        "    deep_inputs = tf.keras.layers.DenseFeatures(\n",
        "        feature_columns=deep_fc.values(), name=\"deep_inputs\")(inputs)\n",
        "\n",
        "    # Get output of model given inputs\n",
        "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
        "\n",
        "    # Build model and compile it all together\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    model = build_wide_deep_model(args[\"nnsize\"], args[\"nembeds\"])\n",
        "    print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
        "    print(model.summary())\n",
        "\n",
        "    trainds = load_dataset(\n",
        "        args[\"train_data_path\"],\n",
        "        args[\"batch_size\"],\n",
        "        'train')\n",
        "\n",
        "    evalds = load_dataset(\n",
        "        args[\"eval_data_path\"], 1000, 'eval')\n",
        "    if args[\"eval_steps\"]:\n",
        "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
        "\n",
        "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
        "    steps_per_epoch = args[\"train_examples\"] // num_batches\n",
        "\n",
        "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/citibike\")\n",
        "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        trainds,\n",
        "        validation_data=evalds,\n",
        "        epochs=args[\"num_epochs\"],\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
        "        callbacks=[cp_callback])\n",
        "\n",
        "    EXPORT_PATH = os.path.join(\n",
        "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
        "    tf.saved_model.save(\n",
        "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
        "    \n",
        "    hp_metric = history.history['val_rmse'][-1]\n",
        "\n",
        "    hpt = hypertune.HyperTune()\n",
        "    hpt.report_hyperparameter_tuning_metric(\n",
        "        hyperparameter_metric_tag='rmse',\n",
        "        metric_value=hp_metric,\n",
        "        global_step=args['num_epochs'])\n",
        "    \n",
        "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting citibike/trainer/model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pawqHOYaUjfg",
        "outputId": "f7c10571-43d3-4b6b-ba63-e5a3a15e2828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "OUTDIR=gs://${BUCKET}/citibike/trained_model_keras\n",
        "JOBID=citibike_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training ${JOBID} \\\n",
        "    --region=${REGION} \\\n",
        "    --module-name=trainer.task \\\n",
        "    --package-path=$(pwd)/citibike/trainer \\\n",
        "    --job-dir=${OUTDIR} \\\n",
        "    --staging-bucket=gs://${BUCKET} \\\n",
        "    --master-machine-type=n1-standard-8 \\\n",
        "    --scale-tier=CUSTOM \\\n",
        "    --runtime-version=${TFVERSION} \\\n",
        "    --python-version=${PYTHONVERSION} \\\n",
        "    -- \\\n",
        "    --train_data_path=gs://${BUCKET}/citibike/data/train*.csv \\\n",
        "    --eval_data_path=gs://${BUCKET}/citibike/data/eval*.csv \\\n",
        "    --output_dir=${OUTDIR} \\\n",
        "    --num_epochs=10 \\\n",
        "    --train_examples=10000 \\\n",
        "    --eval_steps=100 \\\n",
        "    --batch_size=32 \\\n",
        "    --nembeds=8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jobId: citibike_200827_144418\n",
            "state: QUEUED\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Job [citibike_200827_144418] submitted successfully.\n",
            "Your job is still active. You may view the status of your job with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs describe citibike_200827_144418\n",
            "\n",
            "or continue streaming the logs with the command\n",
            "\n",
            "  $ gcloud ai-platform jobs stream-logs citibike_200827_144418\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulG36RqsUjfn",
        "outputId": "d4de26a7-8f53-43e6-f77a-819b9ce30ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%%bash\n",
        "gsutil ls gs://${BUCKET}/citibike/trained_model_keras/20200827150750"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://bq-deploy-ml-pipeline/citibike/trained_model_keras/20200827150750/\n",
            "gs://bq-deploy-ml-pipeline/citibike/trained_model_keras/20200827150750/saved_model.pb\n",
            "gs://bq-deploy-ml-pipeline/citibike/trained_model_keras/20200827150750/assets/\n",
            "gs://bq-deploy-ml-pipeline/citibike/trained_model_keras/20200827150750/variables/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKtb3MGhwlno",
        "outputId": "88fc3e74-f007-4325-f772-3d5d569d70f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "%%bash\n",
        "MODEL_NAME=\"citibike\"\n",
        "MODEL_VERSION=\"keras_on_gcp\"\n",
        "#MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/trained_model/ | tail -1)\n",
        "MODEL_LOCATION=gs://${BUCKET}/citibike/trained_model_keras/20200827150750\n",
        "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
        "#gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
        "#gcloud ai-platform models delete ${MODEL_NAME}\n",
        "#gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
        "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting and deploying citibike keras_on_gcp from gs://bq-deploy-ml-pipeline/citibike/trained_model_keras/20200827150750 ... this will take a few minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using endpoint [https://ml.googleapis.com/]\n",
            "Creating version (this might take a few minutes)......\n",
            "..........................................................................................................................................................................................................................................................................done.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMrL4IKQ0stI",
        "outputId": "f9910c5c-c615-4590-f48e-5471ffa073f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from oauth2client.client import GoogleCredentials\n",
        "import requests\n",
        "import json\n",
        "\n",
        "MODEL_NAME = 'citibike'\n",
        "MODEL_VERSION = 'keras_on_gcp'\n",
        "\n",
        "token = GoogleCredentials.get_application_default().get_access_token().access_token\n",
        "api = 'https://ml.googleapis.com/v1/projects/{}/models/{}/versions/{}:predict' \\\n",
        "         .format(PROJECT, MODEL_NAME, MODEL_VERSION)\n",
        "headers = {'Authorization': 'Bearer ' + token }\n",
        "\n",
        "CSV_COLUMNS = [\"tripduration\",\n",
        "                \"euclidean_diff\",\n",
        "                \"usertype\",\n",
        "                \"birth_year\",\n",
        "                \"gender\"]\n",
        "LABEL_COLUMN = \"tripduration\"\n",
        "\n",
        "\n",
        "data = {\n",
        "  'instances': [\n",
        "      {\n",
        "    \"euclidean_diff\": 0.011828876033106495,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 1952,\n",
        "    \"gender\": \"male\"\n",
        "    },\n",
        "    {\n",
        "    \"euclidean_diff\": 0.011828876033106495,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 1952,\n",
        "    \"gender\": \"female\"\n",
        "    },\n",
        "    {\n",
        "    \"euclidean_diff\": 0.02787824432188638,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 1949,\n",
        "    \"gender\": \"male\"\n",
        "    },\n",
        "    {\n",
        "    \"euclidean_diff\": 0.02787824432188638,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 1949,\n",
        "    \"gender\": \"female\"\n",
        "    },\n",
        "        {\n",
        "    \"euclidean_diff\": 0.02787824432188638,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 1985,\n",
        "    \"gender\": \"male\"\n",
        "    },\n",
        "    {\n",
        "    \"euclidean_diff\": 0.02787824432188638,\n",
        "    \"usertype\": \"Subscriber\",\n",
        "    \"birth_year\": 2000,\n",
        "    \"gender\": \"male\"\n",
        "    },\n",
        "    {\n",
        "    \"euclidean_diff\": 0.02787824432188638,\n",
        "    \"usertype\": \"Customer\",\n",
        "    \"birth_year\": 2000,\n",
        "    \"gender\": \"male\"\n",
        "    },\n",
        "  ]\n",
        "}\n",
        "response = requests.post(api, json=data, headers=headers)\n",
        "print(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'{\"predictions\": [{\"time\": [476.48712158203125]}, {\"time\": [567.864990234375]}, {\"time\": [1241.9930419921875]}, {\"time\": [1333.370849609375]}, {\"time\": [1190.6685791015625]}, {\"time\": [1082.8656005859375]}, {\"time\": [2718.666015625]}]}'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}